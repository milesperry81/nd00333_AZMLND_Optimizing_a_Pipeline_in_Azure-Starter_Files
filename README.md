# Optimizing an ML Pipeline in Azure

## Overview
- This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

- The dataset contains data about direct marketing campaigns of a banking institution. We seek to predict if a banking client will subscribe to a term deposit (variable y).

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

- The best performing model was the Voting Ensemble model generated by AutoML. It was 91.71% accurate versus the best performing Logistic Regression model generated by Hyperdrive, which was 91.23% accurate.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

- A dataset was pulled in from a web URL using the TabularDatasetFactory class. It contained 20 feature columns (e.g. age, job, martial status,...) relating to banking customers and a target column "y" indicating whether or not the customer subscribed to a term deposit. The data was cleaned (e.g. convert to one-hot encodings) and split into train and test sets.
- The training data is passed through a logistic regression algorithm for training. Accuracy is then checked on the test data.
- The Hyperdrive class allows multiple runs of the algorithm using different choices of hyperparameters in order to find the most accurate model. Different combinations of the max number of iterations and an inverse regularisation parameter were tried. 

**What are the benefits of the parameter sampler you chose?**
- I used random parameter sampling to make random hyperparameter choices over the defined hyperparameter space for a maximum number of runs. Random sampling can reduce the number of samples taken across the entire hyperparameter space. Research has shown that it often yields similar results to sampling the entire hyperparameter whilst it conserves computing resources.

**What are the benefits of the early stopping policy you chose?**
- I used a bandit early stopping policy. It checks the run every 2 iterations. If accuracy falls outside the top 10% then it will terminate the run. This saves me time and compute power by cancelling runs that would yield poor accuracy results.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

- AutoML automates the process of choosing a model and hyperparameters for a particular problem. 

- Some example models used by AutoML are: LightGMB, XGBoostClassifier, SGD, RandomForest, ExtremeRandomTrees, Voting Ensemble and Stack Ensemble.

- The best model geneated by AutoML was a Voting Ensemble model. The hyperparameters were: 
	min_samples_leaf=0.06157894736842105,                                                                                           min_samples_split=0.10368421052631578,
	min_weight_fraction_leaf=0.0,
	n_estimators=50,
	n_jobs=1,
	oob_score=False,
	random_state=None,
	verbose=0,
	warm_start=False


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

- The AutoML best model was 91.71% accurate. The Hyperdrive best Logistic Regression model was 91.23% accurate. In all, not much in it, but AutoML was more accurate.

- AutoML had more freedom to choose different machine learning algorithms, whereas the Hyperdrive experient was constrained to use Logistic Regression only. I believe this freedom gave AutoML the opportunity to find a better model than Logistic Regression only. 



## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

- The Hyperdrive experiment was restricted to a single Logistic Regression model only and the choice of hyperparameter was contrained by the hyperparameter space set by myself. I would like to explore a larger hyperparameter space and see if this improves the Logistic Regression model.

- Running AutoML for a greater number of iterations and allowing it to explore more variations in models and hyperparamters may see improvements in the best model accuracy.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

- Please see the notebook.
