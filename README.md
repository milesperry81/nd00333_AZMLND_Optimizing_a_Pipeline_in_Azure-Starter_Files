# Optimizing an ML Pipeline in Azure

## Overview
- This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

- The dataset contains data about direct marketing campaigns of a banking institution. We seek to predict if a banking client will subscribe to a term deposit (variable y).

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

- The best performing model was the Voting Ensemble model generated by AutoML. It was 91.64% accurate versus the best performing Logistic Regression model generated by Hyperdrive, which was 91.23% accurate.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

- A dataset was pulled in from a web URL using the TabularDatasetFactory class. It contained 20 feature columns (e.g. age, job, martial status,...) relating to banking customers and a target column "y" indicating whether or not the customer subscribed to a term deposit. The data was cleaned (e.g. convert to one-hot encodings) and split into train and test sets.
- The training data is passed through a logistic regression algorithm for training. Accuracy is then checked on the test data.
- The Hyperdrive class allows multiple runs of the algorithm using different choices of hyperparameters in order to find the most accurate model. Different combinations of the max number of iterations and an inverse regularisation parameter were tried. 
- The max iterations hyperparameter (max_iter) was passed to the RandomParameterSampling class using the "choice" parameter expression. This instructs the random sampler to sample from a discrete set of values. The set of discrete values that I passed to the sampler were 10, 50 and 100. The inverse regularisation parameter (C) instead used the "uniform" parameter expression. As such the sampler takes random samples from a continuous uniform distribution. The uniform distribution that I passed to the sampler was in the range 1 to 5.

**What are the benefits of the parameter sampler you chose?**
- I used random parameter sampling to make random hyperparameter choices over the defined hyperparameter space for a maximum number of runs. Random sampling can reduce the number of samples taken across the entire hyperparameter space. Research has shown that it often yields similar results to sampling the entire hyperparameter whilst it conserves computing resources.

**What are the benefits of the early stopping policy you chose?**
- I used a Bandit Stopping Policy. It checks the run every 2 iterations. If accuracy falls outside the slack factor (0.1, or top 10%) then it will terminate the run. This saves me time and compute power by cancelling runs that would yield poor accuracy results. Being able to specify the slack factor allows for more aggressive savings with a smaller slack factor than you would get with the Median Stopping Policy, which terminates runs where the primary metric is less than the median of the primary metric averages.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

- AutoML automates the process of choosing a model and hyperparameters for a particular problem. 

- Some example models used by AutoML are: LightGMB, XGBoostClassifier, SGD, RandomForest, ExtremeRandomTrees, Voting Ensemble and Stack Ensemble.

- The best model geneated by AutoML was a Voting Ensemble model. It used 7 different estimators with a soft voting type.

The estimators their corresponding weights were:

'estimators': ['1', '0', '3', '13', '9', '5', '17'],
 'weights': [0.2,
             0.4666666666666667,
             0.06666666666666667,
             0.06666666666666667,
             0.06666666666666667,
             0.06666666666666667,
             0.06666666666666667]

The hyperparameters for the first estimator are below. It used MaxAbsScalar to normalise the features and the XGBoostClassifier to make a prediction. See the notebook and the print_model(model) function for details of other estimators.

1 - maxabsscaler
{'copy': True}

1 - xgboostclassifier
{'base_score': 0.5,
 'booster': 'gbtree',
 'colsample_bylevel': 1,
 'colsample_bynode': 1,
 'colsample_bytree': 1,
 'gamma': 0,
 'learning_rate': 0.1,
 'max_delta_step': 0,
 'max_depth': 3,
 'min_child_weight': 1,
 'missing': nan,
 'n_estimators': 100,
 'n_jobs': 1,
 'nthread': None,
 'objective': 'binary:logistic',
 'random_state': 0,
 'reg_alpha': 0,
 'reg_lambda': 1,
 'scale_pos_weight': 1,
 'seed': None,
 'silent': None,
 'subsample': 1,
 'tree_method': 'auto',
 'verbose': -10,
 'verbosity': 0}

It was noted the DataTransformer pre-processing class was not used and it's paramters were all set to none.

datatransformer
{'enable_dnn': None,
 'enable_feature_sweeping': None,
 'feature_sweeping_config': None,
 'feature_sweeping_timeout': None,
 'featurization_config': None,
 'force_text_dnn': None,
 'is_cross_validation': None,
 'is_onnx_compatible': None,
 'logger': None,
 'observer': None,
 'task': None,
 'working_dir': None}

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

- The AutoML best model was 91.64% accurate. The Hyperdrive best Logistic Regression model was 91.23% accurate. In all, not much in it, but AutoML was more accurate.

- AutoML had more freedom to choose different machine learning algorithms, whereas the Hyperdrive experient was constrained to use Logistic Regression only. I believe this freedom gave AutoML the opportunity to find a better model than Logistic Regression only. 



## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

- The Hyperdrive experiment was restricted to a single Logistic Regression model only and the choice of hyperparameter was contrained by the hyperparameter space set by myself. I would like to explore a larger hyperparameter space and see if this improves the Logistic Regression model.

- Running AutoML for a greater number of iterations and allowing it to explore more variations in models and hyperparamters may see improvements in the best model accuracy.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

- Please see the notebook.
